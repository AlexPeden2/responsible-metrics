[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is version 1.0 of the Responsible Research Assessment and Responsible use of Research Metrics quick guide. The guide was adapted by Alex Peden, University of Edinburgh in 2025 from a module was written by the Bibliometrics Team at the University of Southampton Library in 2024 and version 1 released in February 2025, update to version 1.1 made in October 2025. The original module is shared under a CC BY license so that it may be adapted to suit other institutions’ policies. The module has been developed in R using Quarto and is available on GitHub for remixing https://github.com/UniSotonLibrary/responsible-metrics/\nThe University of Southampton team is;\n\nKate Lapage\nClare Hemmings\nLorrayne Smith\n\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "Section 4.html",
    "href": "Section 4.html",
    "title": "Section 4",
    "section": "",
    "text": "In this section you will explore:\n\nHow to use metrics responsibly as part of your assessment\nLimitations of metrics\n\n\n\nResearch metrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs can be a useful addition to a process but they are a limited measure of a researcher’s overall contribution to their discipline or institution. Metrics should never be used as the sole assessment criteria of data set, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their overall contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nIf you research metrics could help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing\n\nasking for an annotated publication list where a candidate identifies their most significant outputs and explains their significance can be a useful approach here\n\nSpecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates\n\nAsk your research office to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time\n\n\n\n\n\nNot all research outputs will be indexed, and therefore might not be included in the calculation of a research metric\n\nTypically journal articles are indexed, and some books may be indexed, smaller publishers may not index their journals\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences journals\nIt can take up to six months from publication for articles to be indexed\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else\n\nCitations take a long time to accrue, this favours older papers over new papers, and those that have been publishing longer over those earlier in their career\nLack of context:\n\nthere is no way of knowing if citations are positive or negative\nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not\n\nDifferent software/databases count things in different ways so it may lead to results you cannot compare\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal level metric like impact factor to infer the quality onto their article\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are doubts about the consistency or fairness of a particular metric or analysis, don’t use it.\nIf you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is more typical. Citation rates can vary even between closely related sub-discaplines.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should only be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section.\n\n\n\nComplete further sections\n\nSection 5 - Using metrics responsibly in personal applications or evaluations",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 4.html#assessing-people-responsibly-with-research-metrics",
    "href": "Section 4.html#assessing-people-responsibly-with-research-metrics",
    "title": "Section 4",
    "section": "",
    "text": "In this section you will explore:\n\nHow to use metrics responsibly as part of your assessment\nLimitations of metrics\n\n\n\nResearch metrics are a quantitative snapshot of how some of a researcher’s research outputs have performed. There are multiple sources of metrics and many different types of metrics for different types of analysis.\nUsing metrics when assessing research outputs can be a useful addition to a process but they are a limited measure of a researcher’s overall contribution to their discipline or institution. Metrics should never be used as the sole assessment criteria of data set, any evaluation of a researcher’s performance should not be solely based on a set of numbers that don’t fully reflect their overall contribution. Their expert judgment and knowledge of their own research are invaluable and should not be overshadowed by quantitative measures. A narrative approach can more accurately reflect the full breadth and depth of a researcher’s impact.\n\n\n\nIf you research metrics could help you in your decision-making process there are several things you can do to make it fairer for researchers and easier for you to compare the results.\n\nSet a time frame e.g. 5 years,\n\nrestricting to a time frame makes it easier to compare academics at different career stages or researchers who might have had a career break\n\nDefine what you mean by ‘best’ or ‘top’ publication(s) so researchers can focus their statements/evidence and ensure that you are comparing the same thing\n\nasking for an annotated publication list where a candidate identifies their most significant outputs and explains their significance can be a useful approach here\n\nSpecify a particular source to use or a specific metrics to use\n\nSome metrics are software/database specific, some of which are available via subscription only and therefore may not be available to all candidates\n\nAsk your research office to provide metrics for the group of candidates so that everyone will be assessed in the same way at the same time\n\n\n\n\n\nNot all research outputs will be indexed, and therefore might not be included in the calculation of a research metric\n\nTypically journal articles are indexed, and some books may be indexed, smaller publishers may not index their journals\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences journals\nIt can take up to six months from publication for articles to be indexed\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else\n\nCitations take a long time to accrue, this favours older papers over new papers, and those that have been publishing longer over those earlier in their career\nLack of context:\n\nthere is no way of knowing if citations are positive or negative\nvolume of citations vary across disciplines, if you are subject expert you may have the contextual knowledge of what to expect but, if not it can be hard to determine whether it is good or not\n\nDifferent software/databases count things in different ways so it may lead to results you cannot compare\nResearchers may use flawed metrics such as h-index, as a proxy of quality and not a true reflection of contribution\nResearchers may use metrics incorrectly or use the wrong type of metric for the assessment they are making; for example using a journal level metric like impact factor to infer the quality onto their article\n\nThe key to using metrics in an assessment of people is to be consistent and fair. If you are doubts about the consistency or fairness of a particular metric or analysis, don’t use it.\nIf you are not a specialist in a subject area you may not have the contextual knowledge to know if the citation volume given is high, low or expected. In some disciplines 15 citations is very high whereas in others it is more typical. Citation rates can vary even between closely related sub-discaplines.\nResearchers are ultimately more than just their scholarly output so you should never base your decision solely on metrics. These should only be used as part of a number of ways to assess the researcher as a whole before any decision is made.\nThank you for taking the time to complete this section.\n\n\n\nComplete further sections\n\nSection 5 - Using metrics responsibly in personal applications or evaluations",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 2.html",
    "href": "Section 2.html",
    "title": "Section 2",
    "section": "",
    "text": "In this section you will explore:\n\nWhat are research metrics?\nPrinciples of responsible use of research metrics\nFurther reading\n\n\n\nResearch metrics are usually quantitative data that describe an aspect of a research output at a particular time. A research output is anything which disseminates research; a journal article, a book, an exhibition, dataset or software. \nThere are many different metrics each attempting to describe a different aspect of research. Metrics can look at one indiviudal research output, at all the outputs from a single researcher, or at the entire output of a department or University. Other metrics describe features of a publication venue, such as a journal. The majority of metrics primarily focus on research journal articles but there are metrics available for other research output types.\nWhen engaging with research metrics its important to focus on the question you are trying to answer, or the information you are trying to convey, so you can decide if using metrics will be useful and accurate, or not.\n\n\n\nAs research disciplines, research practices, research outputs and the information that needs to be conveyed are infinitely variable, there isn’t one approach to engaging with metrics that can be applied universally. In all research assessment the University expects that a transparent set of both qualitative and quantitative information is used to support and inform expert human academic judgement.\nYou will need to consider what is appropriate for the assessment you are making, and follow these principles:\n\nBe Transparent,\nCommunicate your Evaluation Methods,\nEnsure Suitability and\nContextualise.\n\n\n\nProvide clear information on how any quantitative data will be used in decision making, how that data will be validated or checked by those being assessed, and how the data will be supported by qualitative information. Transparency in the use of research metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nIf you decide to use metrics as part of your assessment process, consider:\n\nWhat metrics will be used? Can you apply these across your whole analysis or candidate pool? (if you can’t, you shouldn’t use these metrics)\nWhere did the data come from? When was the data extracted? (data should come from the same sources on a similar date, particularly if you plan to compare any aspect. Citations accrue all the time so different extraction dates can undermine comparison)\nHow will your chosen metrics be support by qualitative information?\n\n\n\n\nThose leading assessment processes must ensure that the evaluation process, including any additional information to be used not provided by the candidate, is clearly communicated in advance to those being assessed. Any quantitative indicator that will be used from either internal or external sources should be communicated, accessible, validatable and open to scrutiny by those being assessed.\n\n\n\nWhen assessing a person, research metrics should not be used as the sole source of information. Metrics should only be used when necessary and should be used in conjunction with qualitative information (e.g. expert testimony or a narrative description of the significance of a research output). Use a ‘basket of metrics’ rather than focusing on one data type.\nBe aware that individuals are unlikely to be directly comparable due to their career path or circumstances. e.g. length of career, career breaks, part-time working, personal circumstances. You should not compare an early career researcher with a researcher who has a 20 year track record in academic research for example.\nYou should avoid using metrics to make comparisons across diverse groups of individuals. For example don’t use one metric to compare across different research disciplines, if you are comparing across similar sub-disciplines use a normalised metric (e.g. field weighted citation impact).\nEach metric has a specific use, make sure you understand what the metric is telling you before using it. For example, you should not use a metric that describes an aspect of a whole journal catalogue (e.g. Impact Factor) to infer the quality of an individual output or one person’s contribution to an output.\n\n\n\nResearch metrics should be appropriately normalised using an established method (e.g. field-weighted citation impact) or presented with suitable context (e.g. research income range by discipline or sub-discipline).\nThose being assessed should have the opportunity to provide context on their own track record to communicate their career path, career breaks, work pattern or any other relevant factors.\n\n\n\n\nRead the University statement on responsible use of research metrics\nVisit our Responsible Research Assessment & Use of Metrics webpage\nVisit our Responsible Research Assessment & Use of Metrics SharePoint site\nSpecific question? Contact research.cultures@ed.ac.uk\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics responsibly in research assessment\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "Section 2.html#responsible-use-of-research-metrics-overview",
    "href": "Section 2.html#responsible-use-of-research-metrics-overview",
    "title": "Section 2",
    "section": "",
    "text": "In this section you will explore:\n\nWhat are research metrics?\nPrinciples of responsible use of research metrics\nFurther reading\n\n\n\nResearch metrics are usually quantitative data that describe an aspect of a research output at a particular time. A research output is anything which disseminates research; a journal article, a book, an exhibition, dataset or software. \nThere are many different metrics each attempting to describe a different aspect of research. Metrics can look at one indiviudal research output, at all the outputs from a single researcher, or at the entire output of a department or University. Other metrics describe features of a publication venue, such as a journal. The majority of metrics primarily focus on research journal articles but there are metrics available for other research output types.\nWhen engaging with research metrics its important to focus on the question you are trying to answer, or the information you are trying to convey, so you can decide if using metrics will be useful and accurate, or not.\n\n\n\nAs research disciplines, research practices, research outputs and the information that needs to be conveyed are infinitely variable, there isn’t one approach to engaging with metrics that can be applied universally. In all research assessment the University expects that a transparent set of both qualitative and quantitative information is used to support and inform expert human academic judgement.\nYou will need to consider what is appropriate for the assessment you are making, and follow these principles:\n\nBe Transparent,\nCommunicate your Evaluation Methods,\nEnsure Suitability and\nContextualise.\n\n\n\nProvide clear information on how any quantitative data will be used in decision making, how that data will be validated or checked by those being assessed, and how the data will be supported by qualitative information. Transparency in the use of research metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nIf you decide to use metrics as part of your assessment process, consider:\n\nWhat metrics will be used? Can you apply these across your whole analysis or candidate pool? (if you can’t, you shouldn’t use these metrics)\nWhere did the data come from? When was the data extracted? (data should come from the same sources on a similar date, particularly if you plan to compare any aspect. Citations accrue all the time so different extraction dates can undermine comparison)\nHow will your chosen metrics be support by qualitative information?\n\n\n\n\nThose leading assessment processes must ensure that the evaluation process, including any additional information to be used not provided by the candidate, is clearly communicated in advance to those being assessed. Any quantitative indicator that will be used from either internal or external sources should be communicated, accessible, validatable and open to scrutiny by those being assessed.\n\n\n\nWhen assessing a person, research metrics should not be used as the sole source of information. Metrics should only be used when necessary and should be used in conjunction with qualitative information (e.g. expert testimony or a narrative description of the significance of a research output). Use a ‘basket of metrics’ rather than focusing on one data type.\nBe aware that individuals are unlikely to be directly comparable due to their career path or circumstances. e.g. length of career, career breaks, part-time working, personal circumstances. You should not compare an early career researcher with a researcher who has a 20 year track record in academic research for example.\nYou should avoid using metrics to make comparisons across diverse groups of individuals. For example don’t use one metric to compare across different research disciplines, if you are comparing across similar sub-disciplines use a normalised metric (e.g. field weighted citation impact).\nEach metric has a specific use, make sure you understand what the metric is telling you before using it. For example, you should not use a metric that describes an aspect of a whole journal catalogue (e.g. Impact Factor) to infer the quality of an individual output or one person’s contribution to an output.\n\n\n\nResearch metrics should be appropriately normalised using an established method (e.g. field-weighted citation impact) or presented with suitable context (e.g. research income range by discipline or sub-discipline).\nThose being assessed should have the opportunity to provide context on their own track record to communicate their career path, career breaks, work pattern or any other relevant factors.\n\n\n\n\nRead the University statement on responsible use of research metrics\nVisit our Responsible Research Assessment & Use of Metrics webpage\nVisit our Responsible Research Assessment & Use of Metrics SharePoint site\nSpecific question? Contact research.cultures@ed.ac.uk\nThank you for taking the time to complete this section of the course.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics responsibly in research assessment\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Responsible Research Assessment and Use of Research Metrics Quick Guide",
    "section": "",
    "text": "Welcome to this quick guide\nThis self-study quick guide covers the core principles of responsible research assessment including responsible use of research metrics. It can be completed in your own time.\nThis guide is designed for:\n\nAnyone involved in research assessment as part of a recruitment, promotion, redundancy or other appointment, award or recognition processes\nAnyone assessing research or a research track record as part of peer review of outputs or proposals\nAnyone who is interested in how research can be assessed\n\nIf you have any questions please contact Alex Peden, Head of Research Cultures via research.cultures@ed.ac.uk\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Section 1.html",
    "href": "Section 1.html",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nWhat is research assessment?\nWhy is responsible research assessment important?\nInternational collaborations to improve research assessment\n\n\n\nResearch assessment is any process where you are making a judgement on the quality, significance or impact of research conducted by an individual or groups of individuals, or institution. This could be during recruitment, annual review, promotion or redundancy processes, while reviewing proposals for funders, or during internal REF preparations or REF panels.\n\n\n\nResearch metrics, or bibliometrics, are a statistical analysis of aspects of research publications and their characteristics. They are a prominent feature of our research eco-system including in research assessment processes.\nWhen used appropriately metrics can provide valuable insights into aspects of research output. Unfortunately, research metrics are often used uncritically or inappropriately which can cause problems for researchers and research.\nMany research metrics have become a shorthand for research quality, but they aren’t based on the quality of the actual research itself. They are a feature of who funded the research, or where it was published, or how many other people are researching a topic. It isn’t always clear how these metrics have been calculated or what each metric actually represents to most people.\nRelying on research metrics to make decisions about individuals and their research is a risky way to assess them, it isn’t fair to the individual, or the work undertaken to produce the research, and can cause us to overlook important or impactful work. There is evidence that inappropriate use of metrics has influenced our research cultures negatively.\nMaking sure we engage with the aim of the assessment we are undertaking and draw on a variety of information including peer review and our own judgement, is a route to undertaking responsible research assessment and improving our research cultures.\n\n\n\nThere are several international collaborations aiming to improve research assessment and encourage researchers and institutions to reflect on how they use research metrics in these processes.  The University has outlined its approach in its statement on responsible use of research metrics which states our commitment to the principles of DORA, CoARA and the Leiden Manifesto.\nBelow are summaries of two key international initiatives Univeristy of Edinburgh is a signatory of: \nDORA: Declaration of Research Assessment \nDORA is a set of principles for responsible research assessment that were published in 2012. They are designed to ensure that the quality and impact of research outputs are “measured accurately and evaluated wisely”.\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nCoARA: Coalition for Advancing Research Assessment \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives. Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research. This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators”. \nThe University of Edinburgh is a signatory of, and committed to, both DORA and CoARA.\n\n\n\nComplete further sections\n\nSection 2 - Responsible Use of Research Metrics: Overview\nSection 3 - Using metrics responsibly in research assessment\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 1.html#responsible-research-assessment-overview",
    "href": "Section 1.html#responsible-research-assessment-overview",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nWhat is research assessment?\nWhy is responsible research assessment important?\nInternational collaborations to improve research assessment\n\n\n\nResearch assessment is any process where you are making a judgement on the quality, significance or impact of research conducted by an individual or groups of individuals, or institution. This could be during recruitment, annual review, promotion or redundancy processes, while reviewing proposals for funders, or during internal REF preparations or REF panels.\n\n\n\nResearch metrics, or bibliometrics, are a statistical analysis of aspects of research publications and their characteristics. They are a prominent feature of our research eco-system including in research assessment processes.\nWhen used appropriately metrics can provide valuable insights into aspects of research output. Unfortunately, research metrics are often used uncritically or inappropriately which can cause problems for researchers and research.\nMany research metrics have become a shorthand for research quality, but they aren’t based on the quality of the actual research itself. They are a feature of who funded the research, or where it was published, or how many other people are researching a topic. It isn’t always clear how these metrics have been calculated or what each metric actually represents to most people.\nRelying on research metrics to make decisions about individuals and their research is a risky way to assess them, it isn’t fair to the individual, or the work undertaken to produce the research, and can cause us to overlook important or impactful work. There is evidence that inappropriate use of metrics has influenced our research cultures negatively.\nMaking sure we engage with the aim of the assessment we are undertaking and draw on a variety of information including peer review and our own judgement, is a route to undertaking responsible research assessment and improving our research cultures.\n\n\n\nThere are several international collaborations aiming to improve research assessment and encourage researchers and institutions to reflect on how they use research metrics in these processes.  The University has outlined its approach in its statement on responsible use of research metrics which states our commitment to the principles of DORA, CoARA and the Leiden Manifesto.\nBelow are summaries of two key international initiatives Univeristy of Edinburgh is a signatory of: \nDORA: Declaration of Research Assessment \nDORA is a set of principles for responsible research assessment that were published in 2012. They are designed to ensure that the quality and impact of research outputs are “measured accurately and evaluated wisely”.\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nCoARA: Coalition for Advancing Research Assessment \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives. Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research. This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators”. \nThe University of Edinburgh is a signatory of, and committed to, both DORA and CoARA.\n\n\n\nComplete further sections\n\nSection 2 - Responsible Use of Research Metrics: Overview\nSection 3 - Using metrics responsibly in research assessment\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 3.html",
    "href": "Section 3.html",
    "title": "Section 3",
    "section": "",
    "text": "In this this section you will explore:\n\nhow to use metrics responsibly in research assessment\nan example scenario\n\n\n\nIf you are considering using research metrics as part of your assessment process it is important you define clearly the overall question that you are trying to answer. Defining your question or task clearly will help you design the assessment process and understand if research metrics can positively support that decision making process or not.\nOnce you have your overall question or task, it can be useful to break it down further by considering these apsects:\n\nWhat specifically do I want to understand?​\nWill quantitative data help me reach this understanding?\nCan I access relevant or appropriate quantitative data?\nHow will I use the quantitative data in the overall process?​\nWhen is the quantitative data or analysis required by? \nWhat are the limitations of my quantitative data?\n\nWhat specifically do I want to understand?​\nFirst of all you need to think carefully about what it is you are trying to understand, what is the question you are trying to answer. Once you’ve decided on this you can then identify information that might help you to answer that question, and where in an assessment process it would be most useful to request that information from applicants. You can ask people to provide this information in their application e.g. a publication list, or to undertake a task as part of the procecss e.g. a presentation or a research funding plan. If you decide undertaking your own analysis involving research metrics might be useful to the process you can identify sources of data that could be helpful.\nWill quantitative data help me reach this understanding?​\nIn most research assessment processes the question you are trying to answer is usually complex - Will this candidate bring expertise in an area we don’t have expertise to the department? Do they complement the work already ongoing in our department and will they be able to collaborate with out exisiting staff? Has this award nominee delivered research that has changed the research field?\nResearch metrics could support your understanding of part of these questions, but its likely this data will need to be supported by your own assessment of the rigour or significance or approach of a candidate’s research outputs.\nCan I access relevant or appropriate data?​\nThe University has access to a number of databases you can use to access data and prepare an analysis. You may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as comparable as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nHow will I use the data?​\nWhether you are carrying out the analysis yourself or you providing data for someone else to analyse, you should keep a record of how you obtained the data and what you do to it to produce an analysis, so that anyone reading your results can understand what you have done.\nWhen is it required by?​\nThe online sources you will use to find your initial data update continuously. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued. It’s important to be clear and transparent about the sources you use, so always say both when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been. What, if any, assumptions have you made about your data to be able to proceed with your analysis? Be clear about the limitations and biases of your data when you record how you have used it.\n\n\n\nHow might we go about responsibly answering questions about papers in the same discipline or by the same author? Let us take this question as an example\nWhy did paper X do so much better than paper Y?​\nLet’s work it through with some hypothetical data.\nX. A.N.Other (2021) Sample paper titled X, Journal of Things, Citation count: 25\nY. A.N.Other (2023) Sample paper titled Y, Journal of Stuff, Citation count: 10\nFirst - what do we mean by better?\nTo begin with, we need to decide how we are quantifying “better”. Once this definition is decided we can source information to do a comparison.\nDoes “better” mean cited more?\nThis is a very simple assessment and looking at the data we can determine that Paper X has performed “better”. Why?\n\nIt could be because paper X was published in 2021, 2 years before paper Y, and the citations have had more time to accumulate\nIt could be that paper X is published open access while paper Y was not, so X has attracted a wider audience\nIt could be that paper Y is on a topic where there are fewer other researchers or research groups in that field, therefore fewer people that would cite the work. 10 citations could represent a citation from every other research group in that field and therefore might represent a significantly impactful publication\nPaper X could articulate a controversial approach where there is not consensus in the discapline, citations may not be supportive of the content of the paper\nIt could be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention\n\nDoes “better” mean more impact in the discipline?\nIf we are looking at disciplinary impact we cannot tell the impact of a paper from citation count alone, so we need to look for more data. The original data doesn’t tell us where the citation count comes from.\nAs we don’t know the source or date the metrics above were extracted, we need go to a database like Scopus and look at each paper to find more data. There we could find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells us that although Paper X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so both papers have about the same amount of disciplinary impact. Paper X isn’t doing much “better” than Paper Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as publication date, discipline, number of researchers in the field and journal reach. The question you ask, how you define “good” or “impactful” will determine the answer you get. So, take your time to consider what it is you need to understand before embarking on an analysis of metrics. It’s likely that in this example asking the author to provide their own statement describing the significance and rigour of these two papers would be more useful to the assessment than the quantitative analysis.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 3.html#using-metrics-responsibly-in-research-assessment",
    "href": "Section 3.html#using-metrics-responsibly-in-research-assessment",
    "title": "Section 3",
    "section": "",
    "text": "In this this section you will explore:\n\nhow to use metrics responsibly in research assessment\nan example scenario\n\n\n\nIf you are considering using research metrics as part of your assessment process it is important you define clearly the overall question that you are trying to answer. Defining your question or task clearly will help you design the assessment process and understand if research metrics can positively support that decision making process or not.\nOnce you have your overall question or task, it can be useful to break it down further by considering these apsects:\n\nWhat specifically do I want to understand?​\nWill quantitative data help me reach this understanding?\nCan I access relevant or appropriate quantitative data?\nHow will I use the quantitative data in the overall process?​\nWhen is the quantitative data or analysis required by? \nWhat are the limitations of my quantitative data?\n\nWhat specifically do I want to understand?​\nFirst of all you need to think carefully about what it is you are trying to understand, what is the question you are trying to answer. Once you’ve decided on this you can then identify information that might help you to answer that question, and where in an assessment process it would be most useful to request that information from applicants. You can ask people to provide this information in their application e.g. a publication list, or to undertake a task as part of the procecss e.g. a presentation or a research funding plan. If you decide undertaking your own analysis involving research metrics might be useful to the process you can identify sources of data that could be helpful.\nWill quantitative data help me reach this understanding?​\nIn most research assessment processes the question you are trying to answer is usually complex - Will this candidate bring expertise in an area we don’t have expertise to the department? Do they complement the work already ongoing in our department and will they be able to collaborate with out exisiting staff? Has this award nominee delivered research that has changed the research field?\nResearch metrics could support your understanding of part of these questions, but its likely this data will need to be supported by your own assessment of the rigour or significance or approach of a candidate’s research outputs.\nCan I access relevant or appropriate data?​\nThe University has access to a number of databases you can use to access data and prepare an analysis. You may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as comparable as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nHow will I use the data?​\nWhether you are carrying out the analysis yourself or you providing data for someone else to analyse, you should keep a record of how you obtained the data and what you do to it to produce an analysis, so that anyone reading your results can understand what you have done.\nWhen is it required by?​\nThe online sources you will use to find your initial data update continuously. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued. It’s important to be clear and transparent about the sources you use, so always say both when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been. What, if any, assumptions have you made about your data to be able to proceed with your analysis? Be clear about the limitations and biases of your data when you record how you have used it.\n\n\n\nHow might we go about responsibly answering questions about papers in the same discipline or by the same author? Let us take this question as an example\nWhy did paper X do so much better than paper Y?​\nLet’s work it through with some hypothetical data.\nX. A.N.Other (2021) Sample paper titled X, Journal of Things, Citation count: 25\nY. A.N.Other (2023) Sample paper titled Y, Journal of Stuff, Citation count: 10\nFirst - what do we mean by better?\nTo begin with, we need to decide how we are quantifying “better”. Once this definition is decided we can source information to do a comparison.\nDoes “better” mean cited more?\nThis is a very simple assessment and looking at the data we can determine that Paper X has performed “better”. Why?\n\nIt could be because paper X was published in 2021, 2 years before paper Y, and the citations have had more time to accumulate\nIt could be that paper X is published open access while paper Y was not, so X has attracted a wider audience\nIt could be that paper Y is on a topic where there are fewer other researchers or research groups in that field, therefore fewer people that would cite the work. 10 citations could represent a citation from every other research group in that field and therefore might represent a significantly impactful publication\nPaper X could articulate a controversial approach where there is not consensus in the discapline, citations may not be supportive of the content of the paper\nIt could be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention\n\nDoes “better” mean more impact in the discipline?\nIf we are looking at disciplinary impact we cannot tell the impact of a paper from citation count alone, so we need to look for more data. The original data doesn’t tell us where the citation count comes from.\nAs we don’t know the source or date the metrics above were extracted, we need go to a database like Scopus and look at each paper to find more data. There we could find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells us that although Paper X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so both papers have about the same amount of disciplinary impact. Paper X isn’t doing much “better” than Paper Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as publication date, discipline, number of researchers in the field and journal reach. The question you ask, how you define “good” or “impactful” will determine the answer you get. So, take your time to consider what it is you need to understand before embarking on an analysis of metrics. It’s likely that in this example asking the author to provide their own statement describing the significance and rigour of these two papers would be more useful to the assessment than the quantitative analysis.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people responsibly using research metrics\nSection 5 - Using metrics responsibly in your own applications or evaluations",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 5.html",
    "href": "Section 5.html",
    "title": "Section 5",
    "section": "",
    "text": "In this section you will explore: \n\nWhat you can do if you’ve been asked to review metrics as part of an assessment process\nWhat you can do if you’ve been asked to include metrics in an application\nHow to get metrics if you need them\nLimitation of metrics\nGuidance on how to use metrics\n\n\n\nIf you are asked to judge or review metrics as part of a peer review process for a funder, or as part of a recruitment or promotion process for another university, you can use the text below as a response rather than participating in that aspect of the assessment.\n“The University of Edinburgh supports responsible research assessment and is a signatory of DORA and CoARA, therefore I am not able to comment on this aspect of the application”\n\n\n\nMetrics can provide a quantitative snapshot of aspects of a research output, or a group of research outputs.  \nThe production of a research output is a core part of the research lifecycle. However, it’s only one part of that lifecycle, and of your broader role as a researcher and academic. Your knowledge of, and your contribution to, your research area is more comprehensive, complex and nuanced than a simple number or list of outputs.  \nIf you haven’t been asked to provide metrics as part of an application process we suggest you don’t provide them. Metrics don’t supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution to the research, and your contribution as a researcher. The only time you have to use metrics is if the assessment process asks you to provide them, and you can always ask if this is neccessary and offer another route to providing the same information.\n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed, and therefore might not be included in the calculation of a research metric\n\nTypically journal articles are indexed, and some books may be indexed, smaller publishers may not index their journals\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences journals\nIt can take up to six months from publication for articles to be indexed\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else\n\nCitations take a long time to accrue, this favours older papers over new papers, and those that have been publishing longer over those earlier in their career\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\nIf you use metrics you need to use them responsibly. There are many different metrics, for many different purposes, from many different sources. You should use the most appropriate metric according to your needs. Below you’ll find examples (both poor and better) of how metrics could be used to support statements about your research. If you aren’t asked to provide metrics in an application we suggest you don’t provide them, include a narrative statement to explain the significance and reach of your work.\n\n\n\nExample of poor use \nI publish in top journals \nWhy this is bad? There’s no context and it’s a vague statement. It is equating the quality of your individual research output with the overall quality of the whole journal’s catalogue. \nExample of better use \n[citation of paper] is my most highly cited paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better? The statement is specific. There is a source identified and there is a metric that is verifiable. You’ve provided a link to a publication that demonstrates some of the impact the research has had.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric (e.g. field weighted citation impact) which is gives a fairer assessment rather than a count. You could also talk about the impact of your research more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] it has been cited 150 times and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I was invited to join a government working group to explore improving government policy in this area.  \nWhy this is good – You’ve given a source and date for your metrics, you’ve used multiple metrices and you’ve used a normalised metric which makes for a fairer comparison. You’ve explained that your output has led to impact. \n\n\n\nA common question is authors asking for help creating a list of their ‘best’ research publications. How you quantify ‘best’ will depend on the context of what you are applying for, your discipline, and what the assessor has asked for. A more useful way to consider this questions could be to identify your “most significant” research output.\nIt can be useful to identify 3-5 research outputs and annotate them in your publication list emphasising your contribution to the research, the development of the output, and any academic or broader impact the research has had.\nExample of an annotation on a publication list Author A.N, Writer B.P, Researcher V.I, Scientist L.P. Treatment of Latent Infection: A Network Meta-Analysis. Annals of Medicine (2019) 424(8):419-28. “I intellectually conceived and conducted this work, extracted and reviewed the data, and wrote the majority of the paper. I attended an international government expert panel to present this work, which led to its use as the scientific foundation for their global ‘Guidelines on the management of infection’, 2019 (which I contributed to the writing of) and an associated publication.”\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nTest your knowledge",
    "crumbs": [
      "Home",
      "Section 5"
    ]
  },
  {
    "objectID": "Section 5.html#using-metrics-responsibly-in-your-own-applications-or-evaluations",
    "href": "Section 5.html#using-metrics-responsibly-in-your-own-applications-or-evaluations",
    "title": "Section 5",
    "section": "",
    "text": "In this section you will explore: \n\nWhat you can do if you’ve been asked to review metrics as part of an assessment process\nWhat you can do if you’ve been asked to include metrics in an application\nHow to get metrics if you need them\nLimitation of metrics\nGuidance on how to use metrics\n\n\n\nIf you are asked to judge or review metrics as part of a peer review process for a funder, or as part of a recruitment or promotion process for another university, you can use the text below as a response rather than participating in that aspect of the assessment.\n“The University of Edinburgh supports responsible research assessment and is a signatory of DORA and CoARA, therefore I am not able to comment on this aspect of the application”\n\n\n\nMetrics can provide a quantitative snapshot of aspects of a research output, or a group of research outputs.  \nThe production of a research output is a core part of the research lifecycle. However, it’s only one part of that lifecycle, and of your broader role as a researcher and academic. Your knowledge of, and your contribution to, your research area is more comprehensive, complex and nuanced than a simple number or list of outputs.  \nIf you haven’t been asked to provide metrics as part of an application process we suggest you don’t provide them. Metrics don’t supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution to the research, and your contribution as a researcher. The only time you have to use metrics is if the assessment process asks you to provide them, and you can always ask if this is neccessary and offer another route to providing the same information.\n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed, and therefore might not be included in the calculation of a research metric\n\nTypically journal articles are indexed, and some books may be indexed, smaller publishers may not index their journals\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences journals\nIt can take up to six months from publication for articles to be indexed\nContent may have been wrongly affiliated to a researcher or their content wrongly associated with someone else\n\nCitations take a long time to accrue, this favours older papers over new papers, and those that have been publishing longer over those earlier in their career\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\nIf you use metrics you need to use them responsibly. There are many different metrics, for many different purposes, from many different sources. You should use the most appropriate metric according to your needs. Below you’ll find examples (both poor and better) of how metrics could be used to support statements about your research. If you aren’t asked to provide metrics in an application we suggest you don’t provide them, include a narrative statement to explain the significance and reach of your work.\n\n\n\nExample of poor use \nI publish in top journals \nWhy this is bad? There’s no context and it’s a vague statement. It is equating the quality of your individual research output with the overall quality of the whole journal’s catalogue. \nExample of better use \n[citation of paper] is my most highly cited paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better? The statement is specific. There is a source identified and there is a metric that is verifiable. You’ve provided a link to a publication that demonstrates some of the impact the research has had.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric (e.g. field weighted citation impact) which is gives a fairer assessment rather than a count. You could also talk about the impact of your research more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] it has been cited 150 times and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I was invited to join a government working group to explore improving government policy in this area.  \nWhy this is good – You’ve given a source and date for your metrics, you’ve used multiple metrices and you’ve used a normalised metric which makes for a fairer comparison. You’ve explained that your output has led to impact. \n\n\n\nA common question is authors asking for help creating a list of their ‘best’ research publications. How you quantify ‘best’ will depend on the context of what you are applying for, your discipline, and what the assessor has asked for. A more useful way to consider this questions could be to identify your “most significant” research output.\nIt can be useful to identify 3-5 research outputs and annotate them in your publication list emphasising your contribution to the research, the development of the output, and any academic or broader impact the research has had.\nExample of an annotation on a publication list Author A.N, Writer B.P, Researcher V.I, Scientist L.P. Treatment of Latent Infection: A Network Meta-Analysis. Annals of Medicine (2019) 424(8):419-28. “I intellectually conceived and conducted this work, extracted and reviewed the data, and wrote the majority of the paper. I attended an international government expert panel to present this work, which led to its use as the scientific foundation for their global ‘Guidelines on the management of infection’, 2019 (which I contributed to the writing of) and an associated publication.”\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nTest your knowledge",
    "crumbs": [
      "Home",
      "Section 5"
    ]
  }
]