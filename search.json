[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is version 1.0 of the Responsible Research Assessment and Responsible use of Research Metrics quick guide. The guide was adapted by Alex Peden, University of Edinburgh in 2025 from a module was written by the Bibliometrics Team at the University of Southampton Library in 2024 and version 1 released in February 2025, update to version 1.1 made in October 2025. The original module is shared under a CC BY license so that it may be adapted to suit other institutions’ policies. The module has been developed in R using Quarto and is available on GitHub for remixing https://github.com/UniSotonLibrary/responsible-metrics/\nThe University of Southampton team is;\n\nKate Lapage\nClare Hemmings\nLorrayne Smith\n\nSection 3;-\n- Updates to the Way’s to use metrics & examples section to improve clarity - Adding a link to a test your knowledge quiz Section 4;- Adding a link to a test your knowledge quiz\n\n\n\n Back to top",
    "crumbs": [
      "Home",
      "About"
    ]
  },
  {
    "objectID": "Section 3.html",
    "href": "Section 3.html",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nanyone who is interested in how metrics contribute to research assessment;\nanyone who is involved in assessing research where metrics may be used as an assessment tool.\n\nIn this this section you will explore:\n\nguidance on how to use metrics in research assessment;\nan example question.\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions:\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you are trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create data visualisations for you, such as VOSviewer.\nWhatever format you decide on, it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued.\nIt’s important to be clear and transparent about the sources you use, so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you made about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you have used it.\n\n\n\nSo, how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with, we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information to do a like-for-like comparison.\nYou will need to be conscious of:\n\nDate i.e. is one paper newer?\nOpen access status – can one paper be accessed by a wider potential audience?\nJournal – if you unable to explain a difference in metrics between two outputs using the information you have, you may want to look at the publication. For example, it may be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y, so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we cannot tell the impact of a paper from citation count alone, so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach too. The question you ask will determine the answer you get. So, take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "Section 3.html#section-2---using-metrics-in-research-assessment",
    "href": "Section 3.html#section-2---using-metrics-in-research-assessment",
    "title": "Section 2",
    "section": "",
    "text": "This section is designed for:\n\nanyone who is interested in how metrics contribute to research assessment;\nanyone who is involved in assessing research where metrics may be used as an assessment tool.\n\nIn this this section you will explore:\n\nguidance on how to use metrics in research assessment;\nan example question.\n\n\n\nWhen you are assessing research using metrics you must have a question that you are trying to answer or task that you are trying to resolve. This question will help guide you to the correct metrics to use.\nWhen you have your question or task, you should break down into the following questions:\n\nWhat do I need to find out?​\nHow will I use the data?​\nHow do I need to present the results?​\nDo I need to use a specific source?​\nWhen is it required by? \nWhat are the limitations of my data?\n\nWhat do I need to find out?​\nFirst of all you need to think carefully about what it is you are trying to assess.\nOnce you have established this, the next stage is to identify which type of metrics and sources should be used to carry out the assessment.\nHow will I use the data?​\nAre you carrying out the analysis or are you providing data for someone else to analyse? Are you providing a list?\nYou should keep a record of what you do to the data and how you obtained it, so that anyone reading your results can replicate what you have done.\nHow do I need to present the results?​\nHow you present results can inform what source you use. Sometimes a table of results will be sufficient. However, there are also metric tools that can create data visualisations for you, such as VOSviewer.\nWhatever format you decide on, it should be appropriate to the question you are trying to answer.\nDo I need to use a specific source?​\nYou may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as equal as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.\nWhen is it required by? ​\nThe online sources you will use to find your initial data continuously update. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued.\nIt’s important to be clear and transparent about the sources you use, so always say when and where you got your data from.\nWhat are the limitations of my data?\nAll data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been.\nWhat, if any, assumptions have you made about your data to be able to proceed with your analysis?\nBe clear about the limitations of your data when you record how you have used it.\n\n\n\nSo, how might we go about responsibly answering the question in papers in the same discipline or by the same author? Let us take this question.\nWhy did paper x do so much better than paper y?​\nTo begin with, we need to decide how we are quantifying ‘better’.\nOnce this definition is decided you can source metrics and information to do a like-for-like comparison.\nYou will need to be conscious of:\n\nDate i.e. is one paper newer?\nOpen access status – can one paper be accessed by a wider potential audience?\nJournal – if you unable to explain a difference in metrics between two outputs using the information you have, you may want to look at the publication. For example, it may be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention.\n\nLet’s work it through with some hypothetical data.\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 25\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count:10\n\nFirst - what do we mean by better?\nbetter = cited more\nThis is a very simple assessment and looking at the data we can determine that Sample paper titled X has performed better, this will be because the paper is published in 2021, 2 years before Sample paper titled Y, so citations have had more time to accumulate.\nHowever\nbetter=more impact\nIf we are looking at impact we cannot tell the impact of a paper from citation count alone, so we can look for more data. The original data doesn’t tell us where the citation count comes from.\nAs I don’t know the source I will go to a database like Scopus and look at each paper to find more data. There I find the citation count, the Field Weighted Citation Impact and Plum X metrics.\n\nField Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average.\nPlum X metrics are an example of an altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley.\n\nA.N.Other (2021) Sample paper titled X, Journal of Things,\n\nCitation count: 26\nField Weighted Citation Impact: 1.57\nPlum X - Captures: 10\n\nA.N.Other (2023) Sample paper titled Y, Journal of Stuff,\n\nCitation count: 12\nField Weighted Citation Impact: 1.52\nPlum X - Captures: 9\n\nThis tells me that although Sample paper titled X has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so actually both papers have about the same amount of impact so Sample paper titled X isn’t doing much better than Sample paper titled Y at all.\nAs you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as age, discipline and journal reach too. The question you ask will determine the answer you get. So, take your time and don’t make snap decisions.\n\n\n\nComplete further sections\n\nSection 3 - Using metrics in personal applications and evaluations\nSection 4 - Assessing people using metrics\n\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 3"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Responsible Research Assessment and Use of Research Metrics Quick Guide",
    "section": "",
    "text": "Welcome to this quick guide\nThis self-study quick guide covers the core principles of responsible research assessment including responsible use of research metrics. It can be completed in your own time.\nThis guide is designed for:\n\nAnyone involved in research assessment as part of a recruitment, promotion, redundancy or other appointment, award or recognition processes\nAnyone assessing research or a research track record as part of peer review of outputs or proposals\nAnyone who is interested in how research can be assessed\n\nFor help please contact Alex Peden, Head of Research Cultures via research.cultures@ed.ac.uk\n\n\n\n\n Back to top",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "Section 1.html",
    "href": "Section 1.html",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nwhat is research assessment?\nwhy is responsible research assessment important?\n\n\n\nResearch assessment is any process where you are making a judgement on the quality, significance or impact of research conducted by an individual or groups of individuals, or institution. This could be during recruitment, annual review, promotion or redundancy processes, while reviewing proposals for funders, or during internal REF preparations or REF panels.\n\n\n\nResearch metrics, or bibliometrics, represent the quantification of publications and their characteristics. They are a prominent feature of our research eco-system including in research assessment processes.\nWhen used appropriately metrics can provide valuable insights into aspects of research output. Unfortunately, research metrics are often used uncritically which can cause problems for researchers and research.\nMany research metrics have become a shorthand for research quality, but they aren’t based on the quality of the actual research itself. They are a feature of who funded the research, or where it was published, or how many other people are researching a topic. It isn’t always clear how these metrics have been calculated or what each metric actually represents to most people.\nRelying on research metrics to make decisions about individuals and their research isn’t fair to the individual, or the work undertaken to produce the research, and can cause us to overlook important or impactful work. There is evidence that inappropriate use of metrics has influenced our research cultures negatively.\nMaking sure peer-review is fore-most in our assessment of research and researchers is a route to undertaking responsible research assessment and improving our research cultures.",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 1.html#responsible-research-assessment-overview",
    "href": "Section 1.html#responsible-research-assessment-overview",
    "title": "Section 1",
    "section": "",
    "text": "In this first section you will explore:\n\nwhat is research assessment?\nwhy is responsible research assessment important?\n\n\n\nResearch assessment is any process where you are making a judgement on the quality, significance or impact of research conducted by an individual or groups of individuals, or institution. This could be during recruitment, annual review, promotion or redundancy processes, while reviewing proposals for funders, or during internal REF preparations or REF panels.\n\n\n\nResearch metrics, or bibliometrics, represent the quantification of publications and their characteristics. They are a prominent feature of our research eco-system including in research assessment processes.\nWhen used appropriately metrics can provide valuable insights into aspects of research output. Unfortunately, research metrics are often used uncritically which can cause problems for researchers and research.\nMany research metrics have become a shorthand for research quality, but they aren’t based on the quality of the actual research itself. They are a feature of who funded the research, or where it was published, or how many other people are researching a topic. It isn’t always clear how these metrics have been calculated or what each metric actually represents to most people.\nRelying on research metrics to make decisions about individuals and their research isn’t fair to the individual, or the work undertaken to produce the research, and can cause us to overlook important or impactful work. There is evidence that inappropriate use of metrics has influenced our research cultures negatively.\nMaking sure peer-review is fore-most in our assessment of research and researchers is a route to undertaking responsible research assessment and improving our research cultures.",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 1.html#responsible-use-of-research-metrics-overview",
    "href": "Section 1.html#responsible-use-of-research-metrics-overview",
    "title": "Section 1",
    "section": "Responsible use of research metrics overview",
    "text": "Responsible use of research metrics overview\nIn this section you will explore:\n\nwhat are research metrics?\nthe principles of responsible use of research metrics\nguidelines for the responsible use of research metrics\n\n\nWhat are research metrics?\nMetrics are a quantitative snapshot of how research outputs perform. A research output is anything which disseminates research; for instance a journal article, a book, an exhibition or software. Metrics can look at one research output, at the outputs from a single researcher, or at the entire output of a department or University. Other metrics indicate the relevance and quality of a venue, such as a journal, but these are not appropriate indicators to use as a proxy to assess an individual or their output. The majority of metrics primarily focus on journal articles but metrics may be available for other research output types.\nThere are many different types of metrics and they each have a prescribed purpose.\nWhen using metrics it is important to have a question you are trying to answer so you can decide which metrics to use.\nUsing the wrong metric can have real world implications, from making false claims to causing reputational harm, which could impact future career ambitions or cause institutional damage.\n\n\nPrinciples of responsible use of metrics\nAs research disciplines and outputs are so varied there is no set step-by-step guide to carrying out an analysis. Instead, there are a set of principles that should be followed: Transparency, Appropriateness, Equality, Reproducible, and Continual Reassessment.\n\nTransparency \nTransparency in metrics means having an explanation in clear, simple language to ensure end-users understand the data used, its reliability and its limitations. \nEssentially what, where and when: \n\nWhat metrics were used? \nWhere did the data come from (sources)? \nWhen was the assessment made? – The assessment is only a snapshot in time; data sources are updated as indicators mature. \n\n\n\nEquality \nThe use of metrics should be fair.  You should only compare like-for-like i.e. make comparisons within a single discipline, a limited time period, or using a normalised metric.\nConsistency is important. If you can’t apply the same method across your analysis you should not use that method. \nBe aware that individuals may not be directly comparable due to length of service or time out of work, such as for maternity leave.\n\nE.g. you should not compare an early career researcher with a researcher who has 20 years of experience unless you have restricted the comparison to a suitable time frame. \n\n\n\nAppropriateness \nAll metrics should be tailored to the focus of the analysis. Use metrics for their intended purpose only.\n\nFor example, you must not use a journal metric to infer the quality of an individual output or a person’s contribution to research.\n\nMetrics should only be used when necessary and should be used in conjunction with expert testimony rather than in isolation. \nWhen assessing a person, metrics must not be used as the sole source of information. This is especially true for employment status, but also for personal reputation in a formal or informal context. \n\nFor example, a highly cited paper might be highly cited because everyone disagrees with it\n\nIt is recommended that you use more than one metric to verify results. \n\n\nReproducible \nAnyone should be able to reproduce your results by using the explanation you have provided.\n\n\nContinually reassess \nContinually assess commonly used metrics, especially concerning appropriateness and equality. If a metric is no longer fit for purpose, it should not be used. \n\n\n\nGuidelines around metrics\nA number of guidelines have been published which look to shape our approach to responsible metrics.  The University has its own responsible metrics policy which is based on the principles of DORA and the Leiden Manifesto (https://doi.org/10.1038/520429a). Below are summaries of three key international initiatives, each of which advise on a different aspect: \nDORA (the Declaration of Research Assessment) \nDORA is a set of principles that were published in 2012. They are designed to ensure that the quality and impact of scientific outputs is “measured accurately and evaluated wisely”.\nDORA focuses particularly on the use of journal-based metrics. Its key tenet is to “do not use journal-based metrics, such as Journal Impact Factors (JIFs), as surrogate measures of the quality of individual research articles, to assess an individual scientist’s contributions, or in hiring, promotion, or funding decisions”. \nDORA also supports the idea that research should be assessed on its own merits and not influenced by the reputation of the journal in which it has been published. \nCoARA (Coalition of Advancing Research Assessment) \nThe CoARA initiative was launched in January 2022 and builds on DORA and other initiatives. Its core principle is “that in the assessment of research, researchers and research organisations needs to recognise that diverse outputs, practices and activities contribute to the quality and impact of research. This requires basing assessment primarily on qualitative judgement for which peer review is central, supported by the responsible use of quantitative indicators” (metrics). \nThe University of Edinburgh is a signatory of, and committed to, both DORA and CoARA.\nBarcelona Declaration on Open Research Information \nLaunched in April 2024, the Barcelona Declaration has the backing of research and funding organisations who have endorsed its commitment to “make openness the default for the research information we use and produce”. \nOne of its central recommendations is that we should move away from the use of closed and commercial data sources and work towards services and systems that support and enable research information which is open. These open tools should then give access to metrics that have greater transparency and reproducibility. \n\n\nWhere might you see metrics used?\nMetrics are used in a wide range of activities such as:\n\nresearch assessment;\ngrant applications;\nrecruitment;\npromotion;\nleague tables, such as QS.\n\nThank you for taking the time to complete this section of the course.\n\n\nWhat’s next?\nComplete further sections\n\nSection 2 - Using metrics in research assessment.\nSection 3 - Using metrics in personal applications and evaluations.\nSection 4 - Assessing people using metrics.\n\nRead the University statement on responsible use of research metrics\nVisit our Responsible Research Assessment & Use of Metrics webpage\nVisit our Responsible Research Assessment & Use of Metrics SharePoint site\nSpecific question? Contact us at research.cultures@ed.ac.uk\nFurther Resources:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about.\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4.",
    "crumbs": [
      "Home",
      "Section 1"
    ]
  },
  {
    "objectID": "Section 4.html",
    "href": "Section 4.html",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nanyone who is applying for a job or promotion where research metrics have been requested as part of an application\nanyone who is applying for research funding or an award where research metrics have been requested as part of an application\nanyone who is reviewing or assessing applications for jobs, promotions, funding or awards\nanyone who is interested in understanding responsible research assessment and responsible use of research metrics\n\nIn this section you will explore: \n\nwhat you can do if you’ve been asked to include metrics in an application\nwhat you can do if you’ve been asked to review metrics as part of an assessment process\nsources of metrics;\nlimitation of metrics;\nguidance on how to use metrics.\n\n\n\nMetrics can provide a quantitative snapshot of aspects of a research output, or a group of research outputs.  \nThe production of a research output is a core part of the research lifecycle. However, it’s only a part of that lifecycle, and of your broader role as a researcher and academic. Your knowledge of, and your contribution to, your research area is more comprehensive, complex and nuanced than a simple number or list of outputs.  \nIf you haven’t been asked to provide metrics as part of an application process we suggest you don’t provide them. Metrics don’t supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution to the research, and your contribution as a research. The only time you have to use metrics is if the assessment process asks you to provide them, and you can always ask if this is neccessary and offer another route to providing the same information.\n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for your article to be indexed (sometimes even longer!).\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.\n\nCitations take a long time to accumulate.\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\n\nYou could use metrics to evidence statements you make about your research.\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you use metrics you need to use them responsibly. There are many different metrics, for many different purposes, from many different sources. When you make your assessment you should use the most appropriate metric according to your needs. Below you’ll find examples (both good and bad) of how some of these metrics could be used to make statements about your research.\n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] it has been cited 150 times and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metrics, you’ve used multiple metrices and you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo, how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers.\nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content.\nAltmetrics – metrics derived from alternative types of research output, such as citations or acknowledgements on a public-facing website, social media engagement, or attention from the news media are particularly useful when assessing impact.\nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your own research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is rigorous and reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nTest your knowledge\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  },
  {
    "objectID": "Section 4.html#using-metrics-responsibly-in-personal-applications-or-evaluations",
    "href": "Section 4.html#using-metrics-responsibly-in-personal-applications-or-evaluations",
    "title": "Section 3",
    "section": "",
    "text": "This section is designed for: \n\nanyone who is applying for a job or promotion where research metrics have been requested as part of an application\nanyone who is applying for research funding or an award where research metrics have been requested as part of an application\nanyone who is reviewing or assessing applications for jobs, promotions, funding or awards\nanyone who is interested in understanding responsible research assessment and responsible use of research metrics\n\nIn this section you will explore: \n\nwhat you can do if you’ve been asked to include metrics in an application\nwhat you can do if you’ve been asked to review metrics as part of an assessment process\nsources of metrics;\nlimitation of metrics;\nguidance on how to use metrics.\n\n\n\nMetrics can provide a quantitative snapshot of aspects of a research output, or a group of research outputs.  \nThe production of a research output is a core part of the research lifecycle. However, it’s only a part of that lifecycle, and of your broader role as a researcher and academic. Your knowledge of, and your contribution to, your research area is more comprehensive, complex and nuanced than a simple number or list of outputs.  \nIf you haven’t been asked to provide metrics as part of an application process we suggest you don’t provide them. Metrics don’t supersede your expert opinion and knowledge of your research contribution. Instead, a narrative approach may be better able to give the full breadth of your contribution to the research, and your contribution as a research. The only time you have to use metrics is if the assessment process asks you to provide them, and you can always ask if this is neccessary and offer another route to providing the same information.\n\n\n\nYou can obtain metrics by using an indexing database such as:\n\nScopus. + \nWeb of Science. +\nDimensions.*\nGoogle Scholar.*\nOpen Alex.*\n\nSome of these databases are free to use with an account(*) and some of these are accessible via University of Southampton subscriptions(+). They all have a search function which will allow you to search for your name and/or Researcher ID such as ORCID and from there you can review how your research has been indexed by these systems. \nUsing more than one source of metrics is a good practice as systems index at different rate and speeds. \nContact the metrics service for support, with as much notice as possible. \n\n\n\n\nNot all research outputs will be indexed\n\nTypically journal articles and some books may be indexed.\nSTEM journals are more likely to be indexed than Arts, Humanities and Social Sciences.\nSmaller publishers may not index their journals.\nIt can take up to six months from publication for your article to be indexed (sometimes even longer!).\nContent may have been wrongly affiliated to you or your content wrongly affiliated to someone else.\n\nCitations take a long time to accumulate.\nThere is no way of knowing if citations are positive or negative from metrics. Note, some services do perform sentiment analysis on citations.\n\n\n\n\n\nYou could use metrics to evidence statements you make about your research.\nYou may be asked to provide a list such as your ‘top’ or ‘best’ papers.\n\nIf you use metrics you need to use them responsibly. There are many different metrics, for many different purposes, from many different sources. When you make your assessment you should use the most appropriate metric according to your needs. Below you’ll find examples (both good and bad) of how some of these metrics could be used to make statements about your research.\n\n\n\nExample of bad use \nI publish in top journals \nWhy this is bad – there is no context. It is vague. It is equating the quality of your research with the quality of the journal. \nExample of good use \n[citation of paper] is my top performing paper which according to [source such as Scopus or Google Scholar] has been cited 150 times. It has been cited by [policy document]  \nWhy this is better – there is a source identified and there are actual numbers, there is an idea of age of the paper from the citation, and you’ve demonstrated real world impact.  \nThe statement could be made even better by stating the date you checked the number of citations, as metrics accumulate over time. You could use a normalised metric which is gives a fairer assessment rather than a binary count. You could also talk about your impact more. \nExample of best use  \n[citation of paper] is my top performing paper according to Scopus on [date you checked] it has been cited 150 times and has a field-weighted citation index of 4.3 (1 being average). It has been cited by [policy document]. Following its publication I have been asked by several news agencies to provide commentary on similar news stories thereby contributing to knowledge exchange and public engagement.  \nWhy this is good – You’ve given a source and date for your metrics, you’ve used multiple metrices and you’ve used a normalised metric which makes for a fairer comparison. You’ve related your article to real world impact and engagement. \n\n\n\nA common question we see is authors asking for help creating a list of their ‘best’ research publications. How you quantify best will depend on the context of what you are applying for, your discipline, and what the assessor has asked for.\nSome will argue that ‘best’ means the highest cited, but that automatically puts your newest papers at a disadvantage, as citations take time to accumulate. Some will say ‘best’ is most impactful which can be harder to prove. If it hasn’t been defined for you by the people making the assessment, it is important to state how you have defined best and then use relevant evidence.\nSo, how could you evidence your ‘best’ publications? \n\nList of articles and their citation count – this favours your older papers.\nNormalised metrics such as field-weighted citation index– this gives your newer content a fairer chance against older content.\nAltmetrics – metrics derived from alternative types of research output, such as citations or acknowledgements on a public-facing website, social media engagement, or attention from the news media are particularly useful when assessing impact.\nPolicy citations – has your research been used in government policy? \nPatent citations – has your research been used by someone filing a patent? \n\nWhen using metrics to assess research or during an application you should approach your task with as much rigour and mindfulness of methodology as you would when you undertake your own research practices. The answer you will get will depend on the questions you ask. It should be clear as to how you have come to your conclusions so that your analysis is rigorous and reproducible.\nThank you for taking the time to complete this section of the course. If you need additional help and support with your own applications please contact the metrics service for support.\n\n\n\nComplete further sections\n\nSection 4 - Assessing people using metrics - Section 4 is an optional part of this course for people who will take part in the assessment of people such as those on recruitment panels\n\nTest your knowledge\nRead the University responsible metrics policy\nVisit our Libguide on Metrics\nSpecific question? Contact us at eprints@soton.ac.uk\nFurther Resources external to the University:\n\nDeakin Library Metrics Toolkit: https://deakin.libguides.com/research-metrics/about\nWhat are Responsible Metrics by University of Exeter (4 minute Youtube video): https://www.youtube.com/watch?v=kTYb623Slg4",
    "crumbs": [
      "Home",
      "Section 4"
    ]
  }
]