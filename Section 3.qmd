---
title: "Section 3"
format: html
---

## Using metrics responsibly in research assessment

In this this section you will explore:

-   how to use metrics responsibly in research assessment
-   an example scenario

### Assessing Research Responsibly Using Metrics

If you are considering using research metrics as part of your assessment process it is important you define clearly the overall question that you are trying to answer. Defining your question or task clearly will help you design the assessment process and understand if research metrics can positively support that decision making process or not.

Once you have your overall question or task, it can be useful to break it down further by considering these apsects:

-   What specifically do I want to understand?​
-   Will quantitative data help me reach this understanding? 
-   Can I access relevant or appropriate quantitative data?
-   How will I use the quantitative data in the overall process?​
-   When is the quantitative data or analysis required by? 
-   What are the limitations of my quantitative data?

**What specifically do I want to understand?​**

First of all you need to think carefully about what it is you are trying to understand, what is the question you are trying to answer. Once you've decided on this you can then identify information that might help you to answer that question, and where in an assessment process it would be most useful to request that information from applicants. You can ask people to provide this information in their application e.g. a publication list, or to undertake a task as part of the procecss e.g. a presentation or a research funding plan. If you decide undertaking your own analysis involving research metrics might be useful to the process you can identify sources of data that could be helpful.

**Will quantitative data help me reach this understanding?​**

In most research assessment processes the question you are trying to answer is usually complex - Will this candidate bring expertise in an area we don't currently have expertise in to the department? Do they complement the work already ongoing in our department and will they be able to collaborate with out exisiting staff?  Has this award nominee delivered research that has changed the research field?

Research metrics could support your understanding of part of these questions, but its likely this data will need to be supported by your own assessment of the rigour, or significance, or methodological approach of a candidate's research outputs.

**Can I access relevant or appropriate data?​**

The University has access to a number of [databases](https://library.ed.ac.uk/finding-resources/library-databases){target="_blank"} you can use to access data and prepare an analysis. You may be asked to use a particular source to obtain your metrics, this might be so the requestor can ensure that the metrics they are collecting are as comparable as possible. Sometimes you might need to use a particular source because of the specific data available from that source or because of the functions that the source provides.

**How will I use the data?​**

Whether you are carrying out the analysis yourself or you providing data for someone else to analyse, you should keep a record of how you obtained the data and what you do to it to produce an analysis, so that anyone reading your results can understand what you have done. 

**When is it required by?​**

The online sources you will use to find your initial data update continuously. It therefore might be appropriate to wait until nearer your deadline as further citations may be accrued. It's important to be clear and transparent about the sources you use, so always say both when and where you got your data from.

**What are the limitations of my data?**

All data have limitations, you need to think about what might be missing or whether anything has been included which perhaps shouldn’t have been. What, if any, assumptions have you made about your data to be able to proceed with your analysis? Be clear about the limitations and biases of your data when you record how you have used it.

### **Example**

How might we go about responsibly answering questions about papers in the same discipline or by the same author? Let us take this question as an example

*Why did paper X do so much better than paper Y?​*

Let’s work it through with some hypothetical data.

*X. A.N.Other (2021) Sample paper titled X, Journal of Things, Citation count: 25*

*Y. A.N.Other (2023) Sample paper titled Y, Journal of Stuff, Citation count: 10*

**First, what do we mean by better?**

To begin with, we need to decide how we are quantifying "better". Once this definition is decided we can source information to do a comparison.

***Does "better" mean cited more?***

This is a very simple assessment and looking at the data we can determine that *Paper X* has performed "better".  Why? 

-   It could be because *paper X* was published in 2021, 2 years before *paper Y*, and the citations have had more time to accumulate
-   It could be that *paper X* is published open access while *paper Y* was not, so *X* has attracted a wider audience
-   It could be that *paper Y* is on a topic where there are fewer other researchers or research groups in that field, therefore fewer people that would cite the work. 10 citations could represent a citation from every other research group in that field and therefore might represent a highly impactful publication
-   *Paper X* could articulate a controversial approach where there is not consensus in the discipline, citations may not be supportive of the content of the paper
-   It could be that one paper was published in a society journal, while the other was published in a multidisciplinary journal, explaining a difference in attention

***Does "better" mean more impact in the discipline?***

If we are looking at disciplinary impact we cannot tell the impact of a paper from citation count alone, so we need to look for more data. The original data doesn't tell us where the citation count comes from.

As we don’t know the source or date the metrics above were extracted, we need go to a database like Scopus and look at each paper to find more data. There we could find the citation count, the Field Weighted Citation Impact and Plum X metrics.

-   Field Weighted Citation Impact is a normalised metric that allows us to compare papers of different ages and disciplines - 1 is considered normal or average
-   Plum X metrics are an example of altmetrics. These look at social media engagement, such as likes and shares, and engagement in platforms like Mendeley

*A.N.Other (2021) Sample paper titled X, Journal of Things,*

-   Citation count: 26
-   Field Weighted Citation Impact: 1.57
-   Plum X - Captures: 10

*A.N.Other (2023) Sample paper titled Y, Journal of Stuff,*

-   Citation count: 12
-   Field Weighted Citation Impact: 1.52
-   Plum X - Captures: 9

This tells us that although *Paper X* has over double the amount of citations, the two papers have very similar Field Weighted Citation Impacts and captures so both papers have about the same amount of disciplinary impact. *Paper X* isn’t doing much "better" than *Paper Y* at all.

As you can see from this example, citation numbers don’t always give you the full picture. You need to consider other factors, such as publication date, discipline, number of researchers in the field and journal reach. The question you ask, how you define "good" or "impactful" will determine the answer you get. So, take your time to consider what it is you need to understand before embarking on an analysis of metrics. It's likely that in this example asking the author to provide their own statement describing the significance and rigour of these two papers would be more useful to the assessment than the quantitative analysis.

### **What’s next?**

Complete further sections

-   Section 4 - Assessing people responsibly using research metrics
-   Section 5 - Using metrics responsibly in your own applications or evaluations
